{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P59NYU98GCb9"
   },
   "outputs": [],
   "source": [
    "# !pip3 -qq install torch==0.4.1\n",
    "# !pip3 -qq install bokeh==0.13.0\n",
    "# !pip3 -qq install gensim==3.6.0\n",
    "# !pip3 -qq install nltk\n",
    "# !pip3 -qq install scikit-learn==0.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sVtGHmA9aBM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1"
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs"
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X"
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiA2dGmgF1rW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\julia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\julia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V"
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QstS4NO0L97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv"
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTai8Ta0lgwL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0"
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'ADP', 'ADV', 'DET', 'NUM', 'CONJ', 'PRON', 'VERB', 'X', 'PRT', '.', 'ADJ', 'NOUN'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URC1B2nvPGFt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi"
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rWmSToIaeAo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF"
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd"
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XCuxEBVbOY_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-"
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtRbz1SwgEqc"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhsTKZalfih6"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4XsRII5kW5x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 4), (32, 4))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "# shape changes from batch to batch!\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv"
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.tagset_size = tagset_size\n",
    "        self.hidden_dim = lstm_hidden_dim\n",
    "        self.num_layers = lstm_layers_count\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count)\n",
    "        self.hidden2tag = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        seq_len, batch_size = inputs.size() # shape is (seq, batch)\n",
    "        # Reset, otherwise the LSTM will treat a new batch as a continuation of a sequence\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        embeds = self.word_embeddings(inputs) # shape is (seq, batch, emb)\n",
    "        \n",
    "        # (seq, batch, emb) -> (seq, batch, hidden_dim)\n",
    "#         lstm_out, _ = self.lstm(embeds) \n",
    "#         lstm_out, _ = self.lstm(embeds.view(seq_len, batch_size, -1)) # why do we need view?\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        \n",
    "        # (seq, batch, hidden_dim) -> (seq * batch, hidden_dim)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(seq_len * batch_size, self.hidden_dim))\n",
    "\n",
    "#         tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        # (seq * batch, hidden_dim) -> (seq, batch, tagset_size)\n",
    "#         tag_scores = tag_scores.view(seq_len, batch_size, self.tagset_size)\n",
    "        \n",
    "        tag_scores = tag_space.view(seq_len, batch_size, self.tagset_size)\n",
    "        \n",
    "        return tag_scores\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = Variable(next(self.parameters()).data.new(self.num_layers, batch_size, self.hidden_dim))\n",
    "        cell = Variable(next(self.parameters()).data.new(self.num_layers, batch_size, self.hidden_dim))\n",
    "        \n",
    "        return hidden.zero_(), cell.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH"
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB"
   },
   "outputs": [],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 0.087\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy_counts(predictions, correct_tag_indices):\n",
    "    _, indices = torch.max(predictions, 2) # (seq, batch, tagset_size) -> (seq, batch)\n",
    "    seq, batch_size = correct_tag_indices.shape\n",
    "#     pad_mask = torch.where(correct_tag_indices != 0, torch.ones(seq, batch_size), torch.zeros(seq, batch_size))\n",
    "    pad_mask = correct_tag_indices != 0.0 \n",
    "    sum_count = torch.sum(pad_mask)\n",
    "    correct_count = torch.sum((indices == correct_tag_indices) * pad_mask)\n",
    "    \n",
    "    return correct_count.type(FloatTensor), sum_count.type(FloatTensor)\n",
    "\n",
    "correct_tags_count, total_count = compute_accuracy_counts(logits, y_batch)\n",
    "accuracy = correct_tags_count / total_count\n",
    "print(\"Batch accuracy: %4.3f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMUyUm1hgpe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 2.560\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "loss = criterion(logits.permute(1, 2, 0), y_batch.permute(1, 0))\n",
    "print(\"Batch loss: %4.3f\" % loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH"
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FprPQ0gllo7b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                logits = model(X_batch)\n",
    "\n",
    "                loss = criterion(logits.permute(1, 2, 0), y_batch.permute(1, 0))\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                cur_correct_count, cur_sum_count = compute_accuracy_counts(logits, y_batch)\n",
    "\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "                \n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
    "        val_data=None, val_batch_size=None):\n",
    "        \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')\n",
    "        \n",
    "        # prevent overfitting\n",
    "        if train_acc > 0.9999:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pqfbeh1ltEYa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.67754, Accuracy = 78.53%: 100%|████████████████████████████| 572/572 [00:05<00:00, 101.18it/s]\n",
      "[1 / 50]   Val: Loss = 0.35620, Accuracy = 87.89%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 82.16it/s]\n",
      "[2 / 50] Train: Loss = 0.27100, Accuracy = 91.02%: 100%|████████████████████████████| 572/572 [00:05<00:00, 100.74it/s]\n",
      "[2 / 50]   Val: Loss = 0.25475, Accuracy = 91.32%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 80.65it/s]\n",
      "[3 / 50] Train: Loss = 0.18468, Accuracy = 93.87%: 100%|████████████████████████████| 572/572 [00:05<00:00, 100.73it/s]\n",
      "[3 / 50]   Val: Loss = 0.20937, Accuracy = 92.80%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 81.14it/s]\n",
      "[4 / 50] Train: Loss = 0.13855, Accuracy = 95.37%: 100%|████████████████████████████| 572/572 [00:05<00:00, 101.52it/s]\n",
      "[4 / 50]   Val: Loss = 0.18520, Accuracy = 93.59%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 82.68it/s]\n",
      "[5 / 50] Train: Loss = 0.10827, Accuracy = 96.37%: 100%|████████████████████████████| 572/572 [00:05<00:00, 100.49it/s]\n",
      "[5 / 50]   Val: Loss = 0.18420, Accuracy = 93.87%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 82.16it/s]\n",
      "[6 / 50] Train: Loss = 0.08638, Accuracy = 97.09%: 100%|████████████████████████████| 572/572 [00:05<00:00, 102.26it/s]\n",
      "[6 / 50]   Val: Loss = 0.17417, Accuracy = 94.23%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 84.01it/s]\n",
      "[7 / 50] Train: Loss = 0.06946, Accuracy = 97.66%: 100%|████████████████████████████| 572/572 [00:05<00:00, 100.44it/s]\n",
      "[7 / 50]   Val: Loss = 0.17927, Accuracy = 94.29%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 81.65it/s]\n",
      "[8 / 50] Train: Loss = 0.05640, Accuracy = 98.12%: 100%|████████████████████████████| 572/572 [00:05<00:00, 100.10it/s]\n",
      "[8 / 50]   Val: Loss = 0.19505, Accuracy = 94.24%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 61.24it/s]\n",
      "[9 / 50] Train: Loss = 0.04572, Accuracy = 98.48%: 100%|████████████████████████████| 572/572 [00:05<00:00, 101.65it/s]\n",
      "[9 / 50]   Val: Loss = 0.19245, Accuracy = 94.32%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 87.08it/s]\n",
      "[10 / 50] Train: Loss = 0.03724, Accuracy = 98.77%: 100%|███████████████████████████| 572/572 [00:05<00:00, 101.42it/s]\n",
      "[10 / 50]   Val: Loss = 0.20228, Accuracy = 94.37%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 81.40it/s]\n",
      "[11 / 50] Train: Loss = 0.03039, Accuracy = 99.00%: 100%|████████████████████████████| 572/572 [00:05<00:00, 99.47it/s]\n",
      "[11 / 50]   Val: Loss = 0.23110, Accuracy = 94.23%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 80.65it/s]\n",
      "[12 / 50] Train: Loss = 0.02481, Accuracy = 99.20%: 100%|███████████████████████████| 572/572 [00:05<00:00, 100.12it/s]\n",
      "[12 / 50]   Val: Loss = 0.22767, Accuracy = 94.35%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 84.55it/s]\n",
      "[13 / 50] Train: Loss = 0.02007, Accuracy = 99.37%: 100%|███████████████████████████| 572/572 [00:05<00:00, 100.78it/s]\n",
      "[13 / 50]   Val: Loss = 0.26391, Accuracy = 94.17%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 84.82it/s]\n",
      "[14 / 50] Train: Loss = 0.01617, Accuracy = 99.50%: 100%|███████████████████████████| 572/572 [00:05<00:00, 103.85it/s]\n",
      "[14 / 50]   Val: Loss = 0.26668, Accuracy = 94.14%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 84.55it/s]\n",
      "[15 / 50] Train: Loss = 0.01316, Accuracy = 99.61%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.61it/s]\n",
      "[15 / 50]   Val: Loss = 0.29184, Accuracy = 94.12%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 84.01it/s]\n",
      "[16 / 50] Train: Loss = 0.01074, Accuracy = 99.69%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.39it/s]\n",
      "[16 / 50]   Val: Loss = 0.30891, Accuracy = 94.10%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 90.69it/s]\n",
      "[17 / 50] Train: Loss = 0.00908, Accuracy = 99.74%: 100%|███████████████████████████| 572/572 [00:05<00:00, 103.83it/s]\n",
      "[17 / 50]   Val: Loss = 0.31961, Accuracy = 94.07%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 84.01it/s]\n",
      "[18 / 50] Train: Loss = 0.00788, Accuracy = 99.77%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.56it/s]\n",
      "[18 / 50]   Val: Loss = 0.33062, Accuracy = 94.08%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 90.07it/s]\n",
      "[19 / 50] Train: Loss = 0.00704, Accuracy = 99.79%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.02it/s]\n",
      "[19 / 50]   Val: Loss = 0.34715, Accuracy = 94.08%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 85.93it/s]\n",
      "[20 / 50] Train: Loss = 0.00639, Accuracy = 99.81%: 100%|███████████████████████████| 572/572 [00:05<00:00, 102.86it/s]\n",
      "[20 / 50]   Val: Loss = 0.36154, Accuracy = 94.09%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 82.16it/s]\n",
      "[21 / 50] Train: Loss = 0.00565, Accuracy = 99.82%: 100%|███████████████████████████| 572/572 [00:05<00:00, 100.20it/s]\n",
      "[21 / 50]   Val: Loss = 0.36763, Accuracy = 94.05%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 80.66it/s]\n",
      "[22 / 50] Train: Loss = 0.00601, Accuracy = 99.81%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.09it/s]\n",
      "[22 / 50]   Val: Loss = 0.37703, Accuracy = 94.08%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 86.79it/s]\n",
      "[23 / 50] Train: Loss = 0.00551, Accuracy = 99.82%: 100%|███████████████████████████| 572/572 [00:05<00:00, 103.68it/s]\n",
      "[23 / 50]   Val: Loss = 0.38143, Accuracy = 94.12%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 90.38it/s]\n",
      "[24 / 50] Train: Loss = 0.00536, Accuracy = 99.82%: 100%|███████████████████████████| 572/572 [00:05<00:00, 103.29it/s]\n",
      "[24 / 50]   Val: Loss = 0.39705, Accuracy = 94.07%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 87.08it/s]\n",
      "[25 / 50] Train: Loss = 0.00505, Accuracy = 99.83%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.38it/s]\n",
      "[25 / 50]   Val: Loss = 0.39656, Accuracy = 94.04%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 86.79it/s]\n",
      "[26 / 50] Train: Loss = 0.00487, Accuracy = 99.83%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.69it/s]\n",
      "[26 / 50]   Val: Loss = 0.42064, Accuracy = 94.03%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 85.65it/s]\n",
      "[27 / 50] Train: Loss = 0.00519, Accuracy = 99.83%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.81it/s]\n",
      "[27 / 50]   Val: Loss = 0.41828, Accuracy = 94.06%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 87.37it/s]\n",
      "[28 / 50] Train: Loss = 0.00482, Accuracy = 99.83%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.39it/s]\n",
      "[28 / 50]   Val: Loss = 0.41601, Accuracy = 94.09%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 87.96it/s]\n",
      "[29 / 50] Train: Loss = 0.00502, Accuracy = 99.82%: 100%|███████████████████████████| 572/572 [00:05<00:00, 105.32it/s]\n",
      "[29 / 50]   Val: Loss = 0.40521, Accuracy = 94.18%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 89.47it/s]\n",
      "[30 / 50] Train: Loss = 0.00477, Accuracy = 99.83%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.68it/s]\n",
      "[30 / 50]   Val: Loss = 0.44460, Accuracy = 94.04%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 86.80it/s]\n",
      "[31 / 50] Train: Loss = 0.00429, Accuracy = 99.84%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.27it/s]\n",
      "[31 / 50]   Val: Loss = 0.42660, Accuracy = 94.12%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 90.07it/s]\n",
      "[32 / 50] Train: Loss = 0.00422, Accuracy = 99.84%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.42it/s]\n",
      "[32 / 50]   Val: Loss = 0.41148, Accuracy = 94.10%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 86.79it/s]\n",
      "[33 / 50] Train: Loss = 0.00450, Accuracy = 99.83%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.00it/s]\n",
      "[33 / 50]   Val: Loss = 0.44198, Accuracy = 94.06%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 84.82it/s]\n",
      "[34 / 50] Train: Loss = 0.00553, Accuracy = 99.80%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.49it/s]\n",
      "[34 / 50]   Val: Loss = 0.41796, Accuracy = 94.06%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 85.93it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[35 / 50] Train: Loss = 0.00465, Accuracy = 99.82%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.72it/s]\n",
      "[35 / 50]   Val: Loss = 0.41905, Accuracy = 94.18%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 86.22it/s]\n",
      "[36 / 50] Train: Loss = 0.00399, Accuracy = 99.84%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.60it/s]\n",
      "[36 / 50]   Val: Loss = 0.43066, Accuracy = 94.23%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 89.15it/s]\n",
      "[37 / 50] Train: Loss = 0.00396, Accuracy = 99.84%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.70it/s]\n",
      "[37 / 50]   Val: Loss = 0.43761, Accuracy = 94.15%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 87.38it/s]\n",
      "[38 / 50] Train: Loss = 0.00394, Accuracy = 99.85%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.30it/s]\n",
      "[38 / 50]   Val: Loss = 0.45582, Accuracy = 94.08%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 85.93it/s]\n",
      "[39 / 50] Train: Loss = 0.00390, Accuracy = 99.84%: 100%|███████████████████████████| 572/572 [00:05<00:00, 105.37it/s]\n",
      "[39 / 50]   Val: Loss = 0.44170, Accuracy = 94.18%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 88.25it/s]\n",
      "[40 / 50] Train: Loss = 0.00572, Accuracy = 99.79%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.80it/s]\n",
      "[40 / 50]   Val: Loss = 0.44727, Accuracy = 93.98%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 84.83it/s]\n",
      "[41 / 50] Train: Loss = 0.00498, Accuracy = 99.81%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.41it/s]\n",
      "[41 / 50]   Val: Loss = 0.45006, Accuracy = 94.14%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 84.84it/s]\n",
      "[42 / 50] Train: Loss = 0.00389, Accuracy = 99.85%: 100%|███████████████████████████| 572/572 [00:05<00:00, 102.86it/s]\n",
      "[42 / 50]   Val: Loss = 0.43618, Accuracy = 94.20%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 84.01it/s]\n",
      "[43 / 50] Train: Loss = 0.00373, Accuracy = 99.85%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.38it/s]\n",
      "[43 / 50]   Val: Loss = 0.45249, Accuracy = 94.19%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 84.82it/s]\n",
      "[44 / 50] Train: Loss = 0.00376, Accuracy = 99.85%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.83it/s]\n",
      "[44 / 50]   Val: Loss = 0.46505, Accuracy = 94.14%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 85.37it/s]\n",
      "[45 / 50] Train: Loss = 0.00379, Accuracy = 99.84%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.39it/s]\n",
      "[45 / 50]   Val: Loss = 0.44389, Accuracy = 94.21%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 87.38it/s]\n",
      "[46 / 50] Train: Loss = 0.00459, Accuracy = 99.82%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.82it/s]\n",
      "[46 / 50]   Val: Loss = 0.45164, Accuracy = 94.07%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 87.66it/s]\n",
      "[47 / 50] Train: Loss = 0.00553, Accuracy = 99.79%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.09it/s]\n",
      "[47 / 50]   Val: Loss = 0.45663, Accuracy = 94.07%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 89.45it/s]\n",
      "[48 / 50] Train: Loss = 0.00394, Accuracy = 99.84%: 100%|███████████████████████████| 572/572 [00:05<00:00, 103.73it/s]\n",
      "[48 / 50]   Val: Loss = 0.46469, Accuracy = 94.10%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 85.10it/s]\n",
      "[49 / 50] Train: Loss = 0.00371, Accuracy = 99.85%: 100%|███████████████████████████| 572/572 [00:05<00:00, 104.07it/s]\n",
      "[49 / 50]   Val: Loss = 0.45093, Accuracy = 94.18%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 85.93it/s]\n",
      "[50 / 50] Train: Loss = 0.00367, Accuracy = 99.85%: 100%|████████████████████████████| 572/572 [00:05<00:00, 99.54it/s]\n",
      "[50 / 50]   Val: Loss = 0.45817, Accuracy = 94.14%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 79.18it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5"
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5"
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98wr38_rw55D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      Loss = 0.44650, Accuracy = 94.30%: 100%|█████████████████████████████████████████| 28/28 [00:00<00:00, 76.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.44649648134197506, tensor(0.9430, device='cuda:0'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_epoch(model, criterion, data=(X_test, y_test), batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG"
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super(BiLSTMTagger, self).__init__()\n",
    "        self.tagset_size = tagset_size\n",
    "        self.hidden_dim = lstm_hidden_dim\n",
    "        self.num_layers = lstm_layers_count\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim // 2, lstm_layers_count, bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        seq_len, batch_size = inputs.size() # shape is (seq, batch)\n",
    "        \n",
    "        # Reset, otherwise the LSTM will treat a new batch as a continuation of a sequence\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        embeds = self.word_embeddings(inputs) # shape is (seq, batch, emb)\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden) # (seq, batch, emb) -> (seq, batch, hidden_dim)\n",
    "        \n",
    "        # (seq, batch, hidden_dim) -> (seq * batch, hidden_dim)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(seq_len * batch_size, -1)) \n",
    "\n",
    "#         tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        # (seq * batch, hidden_dim) -> (seq, batch, tagset_size)\n",
    "#         tag_scores = tag_scores.view(seq_len, batch_size, self.tagset_size)\n",
    "        \n",
    "        tag_scores = tag_space.view(seq_len, batch_size, self.tagset_size)\n",
    "        \n",
    "        return tag_scores\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = Variable(next(self.parameters()).data.new(2 * self.num_layers, batch_size, self.hidden_dim // 2))\n",
    "        cell = Variable(next(self.parameters()).data.new(2 * self.num_layers, batch_size, self.hidden_dim // 2))\n",
    "        \n",
    "        return hidden.zero_(), cell.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 35] Train: Loss = 0.63266, Accuracy = 80.41%: 100%|█████████████████████████████| 572/572 [00:06<00:00, 86.05it/s]\n",
      "[1 / 35]   Val: Loss = 0.29710, Accuracy = 90.51%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 79.92it/s]\n",
      "[2 / 35] Train: Loss = 0.21840, Accuracy = 93.11%: 100%|█████████████████████████████| 572/572 [00:06<00:00, 86.44it/s]\n",
      "[2 / 35]   Val: Loss = 0.19340, Accuracy = 93.96%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 83.47it/s]\n",
      "[3 / 35] Train: Loss = 0.14055, Accuracy = 95.65%: 100%|█████████████████████████████| 572/572 [00:06<00:00, 85.75it/s]\n",
      "[3 / 35]   Val: Loss = 0.15228, Accuracy = 95.24%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 87.95it/s]\n",
      "[4 / 35] Train: Loss = 0.09949, Accuracy = 96.97%: 100%|█████████████████████████████| 572/572 [00:06<00:00, 87.75it/s]\n",
      "[4 / 35]   Val: Loss = 0.13355, Accuracy = 95.79%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 91.02it/s]\n",
      "[5 / 35] Train: Loss = 0.07309, Accuracy = 97.81%: 100%|█████████████████████████████| 572/572 [00:06<00:00, 88.69it/s]\n",
      "[5 / 35]   Val: Loss = 0.12293, Accuracy = 96.17%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 91.01it/s]\n",
      "[6 / 35] Train: Loss = 0.05403, Accuracy = 98.40%: 100%|█████████████████████████████| 572/572 [00:06<00:00, 89.40it/s]\n",
      "[6 / 35]   Val: Loss = 0.11944, Accuracy = 96.25%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 87.37it/s]\n",
      "[7 / 35] Train: Loss = 0.04020, Accuracy = 98.83%: 100%|█████████████████████████████| 572/572 [00:06<00:00, 89.79it/s]\n",
      "[7 / 35]   Val: Loss = 0.11615, Accuracy = 96.46%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 90.38it/s]\n",
      "[8 / 35] Train: Loss = 0.02997, Accuracy = 99.15%: 100%|█████████████████████████████| 572/572 [00:06<00:00, 89.67it/s]\n",
      "[8 / 35]   Val: Loss = 0.11814, Accuracy = 96.49%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 89.15it/s]\n",
      "[9 / 35] Train: Loss = 0.02246, Accuracy = 99.38%: 100%|█████████████████████████████| 572/572 [00:06<00:00, 89.59it/s]\n",
      "[9 / 35]   Val: Loss = 0.11917, Accuracy = 96.58%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 90.38it/s]\n",
      "[10 / 35] Train: Loss = 0.01683, Accuracy = 99.55%: 100%|████████████████████████████| 572/572 [00:06<00:00, 89.18it/s]\n",
      "[10 / 35]   Val: Loss = 0.12587, Accuracy = 96.48%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 91.96it/s]\n",
      "[11 / 35] Train: Loss = 0.01250, Accuracy = 99.68%: 100%|████████████████████████████| 572/572 [00:06<00:00, 89.35it/s]\n",
      "[11 / 35]   Val: Loss = 0.13207, Accuracy = 96.53%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 88.86it/s]\n",
      "[12 / 35] Train: Loss = 0.00934, Accuracy = 99.78%: 100%|████████████████████████████| 572/572 [00:06<00:00, 89.48it/s]\n",
      "[12 / 35]   Val: Loss = 0.13697, Accuracy = 96.46%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 87.66it/s]\n",
      "[13 / 35] Train: Loss = 0.00679, Accuracy = 99.85%: 100%|████████████████████████████| 572/572 [00:06<00:00, 88.56it/s]\n",
      "[13 / 35]   Val: Loss = 0.14379, Accuracy = 96.45%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 87.95it/s]\n",
      "[14 / 35] Train: Loss = 0.00505, Accuracy = 99.89%: 100%|████████████████████████████| 572/572 [00:06<00:00, 88.88it/s]\n",
      "[14 / 35]   Val: Loss = 0.14780, Accuracy = 96.42%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 88.25it/s]\n",
      "[15 / 35] Train: Loss = 0.00355, Accuracy = 99.93%: 100%|████████████████████████████| 572/572 [00:06<00:00, 88.31it/s]\n",
      "[15 / 35]   Val: Loss = 0.15547, Accuracy = 96.49%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 87.37it/s]\n",
      "[16 / 35] Train: Loss = 0.00270, Accuracy = 99.95%: 100%|████████████████████████████| 572/572 [00:06<00:00, 86.89it/s]\n",
      "[16 / 35]   Val: Loss = 0.16416, Accuracy = 96.49%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 88.85it/s]\n",
      "[17 / 35] Train: Loss = 0.00191, Accuracy = 99.97%: 100%|████████████████████████████| 572/572 [00:06<00:00, 88.72it/s]\n",
      "[17 / 35]   Val: Loss = 0.16486, Accuracy = 96.51%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 88.85it/s]\n",
      "[18 / 35] Train: Loss = 0.00146, Accuracy = 99.98%: 100%|████████████████████████████| 572/572 [00:06<00:00, 88.21it/s]\n",
      "[18 / 35]   Val: Loss = 0.17465, Accuracy = 96.42%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 91.01it/s]\n",
      "[19 / 35] Train: Loss = 0.00108, Accuracy = 99.99%: 100%|████████████████████████████| 572/572 [00:06<00:00, 89.25it/s]\n",
      "[19 / 35]   Val: Loss = 0.17691, Accuracy = 96.48%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 87.95it/s]\n",
      "[20 / 35] Train: Loss = 0.00108, Accuracy = 99.98%: 100%|████████████████████████████| 572/572 [00:06<00:00, 89.51it/s]\n",
      "[20 / 35]   Val: Loss = 0.18830, Accuracy = 96.31%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 92.94it/s]\n",
      "[21 / 35] Train: Loss = 0.00133, Accuracy = 99.97%: 100%|████████████████████████████| 572/572 [00:06<00:00, 89.23it/s]\n",
      "[21 / 35]   Val: Loss = 0.19238, Accuracy = 96.48%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 87.37it/s]\n",
      "[22 / 35] Train: Loss = 0.00093, Accuracy = 99.99%: 100%|████████████████████████████| 572/572 [00:06<00:00, 88.72it/s]\n",
      "[22 / 35]   Val: Loss = 0.19372, Accuracy = 96.43%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 87.95it/s]\n",
      "[23 / 35] Train: Loss = 0.00065, Accuracy = 99.99%: 100%|████████████████████████████| 572/572 [00:06<00:00, 88.42it/s]\n",
      "[23 / 35]   Val: Loss = 0.19803, Accuracy = 96.44%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 85.93it/s]\n",
      "[24 / 35] Train: Loss = 0.00033, Accuracy = 100.00%: 100%|███████████████████████████| 572/572 [00:06<00:00, 88.58it/s]\n",
      "[24 / 35]   Val: Loss = 0.20046, Accuracy = 96.47%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 85.93it/s]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = BiLSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=35,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm"
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf"
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsCstxiO03oT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know 38736 out of 45441 word embeddings\n"
     ]
    }
   ],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3"
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxaRBpQd0pat"
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super(LSTMTaggerWithPretrainedEmbs, self).__init__()\n",
    "        self.tagset_size = tagset_size\n",
    "        self.hidden_dim = lstm_hidden_dim\n",
    "        self.num_layers = lstm_layers_count\n",
    "        # unfreeze the training so that zero embeddings will be different after training (if there are any)\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(embeddings, freeze=False)\n",
    "        self.lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim // 2, lstm_layers_count, bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        seq_len, batch_size = inputs.size() # shape is (seq, batch)\n",
    "        \n",
    "        # Reset, otherwise the LSTM will treat a new batch as a continuation of a sequence\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        embeds = self.word_embeddings(inputs) # shape is (seq, batch, emb)\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden) # (seq, batch, emb) -> (seq, batch, hidden_dim)\n",
    "        \n",
    "        # (seq, batch, hidden_dim) -> (seq * batch, hidden_dim)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(seq_len * batch_size, -1)) \n",
    "\n",
    "#         tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        # (seq * batch, hidden_dim) -> (seq, batch, tagset_size)\n",
    "#         tag_scores = tag_scores.view(seq_len, batch_size, self.tagset_size)\n",
    "        \n",
    "        tag_scores = tag_space.view(seq_len, batch_size, self.tagset_size)\n",
    "        \n",
    "        return tag_scores\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = Variable(next(self.parameters()).data.new(2 * self.num_layers, batch_size, self.hidden_dim // 2))\n",
    "        cell = Variable(next(self.parameters()).data.new(2 * self.num_layers, batch_size, self.hidden_dim // 2))\n",
    "        \n",
    "        return hidden.zero_(), cell.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBtI6BDE-Fc7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 35] Train: Loss = 0.50922, Accuracy = 86.31%: 100%|█████████████████████████████| 572/572 [00:06<00:00, 88.07it/s]\n",
      "[1 / 35]   Val: Loss = 0.15229, Accuracy = 95.92%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 112.02it/s]\n",
      "[2 / 35] Train: Loss = 0.08681, Accuracy = 97.44%: 100%|█████████████████████████████| 572/572 [00:06<00:00, 88.44it/s]\n",
      "[2 / 35]   Val: Loss = 0.11457, Accuracy = 96.90%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 110.59it/s]\n",
      "[3 / 35] Train: Loss = 0.05528, Accuracy = 98.29%: 100%|█████████████████████████████| 572/572 [00:06<00:00, 88.19it/s]\n",
      "[3 / 35]   Val: Loss = 0.09988, Accuracy = 97.23%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 111.06it/s]\n",
      "[4 / 35] Train: Loss = 0.04065, Accuracy = 98.73%: 100%|█████████████████████████████| 572/572 [00:06<00:00, 90.23it/s]\n",
      "[4 / 35]   Val: Loss = 0.09194, Accuracy = 97.29%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 117.01it/s]\n",
      "[5 / 35] Train: Loss = 0.03200, Accuracy = 99.00%: 100%|█████████████████████████████| 572/572 [00:06<00:00, 92.45it/s]\n",
      "[5 / 35]   Val: Loss = 0.09399, Accuracy = 97.32%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 119.14it/s]\n",
      "[6 / 35] Train: Loss = 0.02603, Accuracy = 99.17%: 100%|█████████████████████████████| 572/572 [00:06<00:00, 92.72it/s]\n",
      "[6 / 35]   Val: Loss = 0.09480, Accuracy = 97.28%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 115.97it/s]\n",
      "[7 / 35] Train: Loss = 0.02156, Accuracy = 99.32%: 100%|█████████████████████████████| 572/572 [00:06<00:00, 92.08it/s]\n",
      "[7 / 35]   Val: Loss = 0.09872, Accuracy = 97.15%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 116.49it/s]\n",
      "[8 / 35] Train: Loss = 0.01782, Accuracy = 99.44%: 100%|█████████████████████████████| 572/572 [00:06<00:00, 92.84it/s]\n",
      "[8 / 35]   Val: Loss = 0.10635, Accuracy = 97.11%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 119.68it/s]\n",
      "[9 / 35] Train: Loss = 0.01476, Accuracy = 99.55%: 100%|█████████████████████████████| 572/572 [00:06<00:00, 92.70it/s]\n",
      "[9 / 35]   Val: Loss = 0.10908, Accuracy = 97.03%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 118.07it/s]\n",
      "[10 / 35] Train: Loss = 0.01218, Accuracy = 99.63%: 100%|████████████████████████████| 572/572 [00:06<00:00, 92.17it/s]\n",
      "[10 / 35]   Val: Loss = 0.12063, Accuracy = 96.92%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 120.78it/s]\n",
      "[11 / 35] Train: Loss = 0.00995, Accuracy = 99.71%: 100%|████████████████████████████| 572/572 [00:06<00:00, 92.50it/s]\n",
      "[11 / 35]   Val: Loss = 0.12519, Accuracy = 96.87%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 120.23it/s]\n",
      "[12 / 35] Train: Loss = 0.00800, Accuracy = 99.78%: 100%|████████████████████████████| 572/572 [00:06<00:00, 92.98it/s]\n",
      "[12 / 35]   Val: Loss = 0.13613, Accuracy = 96.83%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 119.68it/s]\n",
      "[13 / 35] Train: Loss = 0.00644, Accuracy = 99.83%: 100%|████████████████████████████| 572/572 [00:06<00:00, 91.90it/s]\n",
      "[13 / 35]   Val: Loss = 0.14139, Accuracy = 96.77%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 116.49it/s]\n",
      "[14 / 35] Train: Loss = 0.00516, Accuracy = 99.87%: 100%|████████████████████████████| 572/572 [00:06<00:00, 91.36it/s]\n",
      "[14 / 35]   Val: Loss = 0.15676, Accuracy = 96.63%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 120.25it/s]\n",
      "[15 / 35] Train: Loss = 0.00409, Accuracy = 99.90%: 100%|████████████████████████████| 572/572 [00:06<00:00, 92.13it/s]\n",
      "[15 / 35]   Val: Loss = 0.16212, Accuracy = 96.60%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 115.97it/s]\n",
      "[16 / 35] Train: Loss = 0.00322, Accuracy = 99.92%: 100%|████████████████████████████| 572/572 [00:06<00:00, 92.18it/s]\n",
      "[16 / 35]   Val: Loss = 0.16806, Accuracy = 96.57%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 118.60it/s]\n",
      "[17 / 35] Train: Loss = 0.00251, Accuracy = 99.94%: 100%|████████████████████████████| 572/572 [00:06<00:00, 92.54it/s]\n",
      "[17 / 35]   Val: Loss = 0.18594, Accuracy = 96.49%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 118.08it/s]\n",
      "[18 / 35] Train: Loss = 0.00197, Accuracy = 99.96%: 100%|████████████████████████████| 572/572 [00:06<00:00, 92.02it/s]\n",
      "[18 / 35]   Val: Loss = 0.18511, Accuracy = 96.46%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 113.46it/s]\n",
      "[19 / 35] Train: Loss = 0.00159, Accuracy = 99.97%: 100%|████████████████████████████| 572/572 [00:06<00:00, 92.61it/s]\n",
      "[19 / 35]   Val: Loss = 0.19766, Accuracy = 96.37%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 117.01it/s]\n",
      "[20 / 35] Train: Loss = 0.00123, Accuracy = 99.98%: 100%|████████████████████████████| 572/572 [00:06<00:00, 92.58it/s]\n",
      "[20 / 35]   Val: Loss = 0.20856, Accuracy = 96.32%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 116.49it/s]\n",
      "[21 / 35] Train: Loss = 0.00101, Accuracy = 99.98%: 100%|████████████████████████████| 572/572 [00:06<00:00, 92.09it/s]\n",
      "[21 / 35]   Val: Loss = 0.21319, Accuracy = 96.30%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 117.53it/s]\n",
      "[22 / 35] Train: Loss = 0.00085, Accuracy = 99.98%: 100%|████████████████████████████| 572/572 [00:06<00:00, 92.50it/s]\n",
      "[22 / 35]   Val: Loss = 0.22006, Accuracy = 96.30%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 121.91it/s]\n",
      "[23 / 35] Train: Loss = 0.00074, Accuracy = 99.99%: 100%|████████████████████████████| 572/572 [00:06<00:00, 92.43it/s]\n",
      "[23 / 35]   Val: Loss = 0.23325, Accuracy = 96.18%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 124.22it/s]\n",
      "[24 / 35] Train: Loss = 0.00060, Accuracy = 99.99%: 100%|████████████████████████████| 572/572 [00:06<00:00, 92.23it/s]\n",
      "[24 / 35]   Val: Loss = 0.22447, Accuracy = 96.30%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 121.34it/s]\n",
      "[25 / 35] Train: Loss = 0.00064, Accuracy = 99.99%: 100%|████████████████████████████| 572/572 [00:06<00:00, 92.36it/s]\n",
      "[25 / 35]   Val: Loss = 0.23522, Accuracy = 96.24%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 116.49it/s]\n",
      "[26 / 35] Train: Loss = 0.00059, Accuracy = 99.99%: 100%|████████████████████████████| 572/572 [00:06<00:00, 88.27it/s]\n",
      "[26 / 35]   Val: Loss = 0.22509, Accuracy = 96.39%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 112.01it/s]\n",
      "[27 / 35] Train: Loss = 0.00035, Accuracy = 100.00%: 100%|███████████████████████████| 572/572 [00:06<00:00, 88.27it/s]\n",
      "[27 / 35]   Val: Loss = 0.24460, Accuracy = 96.18%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 110.12it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=torch.tensor(embeddings).type(FloatTensor),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=35,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg"
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      Loss = 0.24434, Accuracy = 96.22%: 100%|████████████████████████████████████████| 28/28 [00:00<00:00, 102.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.24434445479086467, tensor(0.9622, device='cuda:0'))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_epoch(model, criterion, data=(X_test, y_test), batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super(LSTMTaggerWithPretrainedEmbs, self).__init__()\n",
    "        self.tagset_size = tagset_size\n",
    "        self.hidden_dim = lstm_hidden_dim\n",
    "        self.num_layers = lstm_layers_count\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(embeddings)\n",
    "        self.lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim, lstm_layers_count)\n",
    "        self.hidden2tag = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        seq_len, batch_size = inputs.size() # shape is (seq, batch)\n",
    "        embeds = torch.where(inputs in w2v_model.vocab, w2v_model.get_vector(inputs),\\\n",
    "                             Variable(next(self.word_embeddings.parameters()).data\\\n",
    "                                      .new(self.word_embeddings.shape[0])).zero_())\n",
    "        # Reset, otherwise the LSTM will treat a new batch as a continuation of a sequence\n",
    "#         self.hidden = self.init_hidden(batch_size) \n",
    "        embeds = self.word_embeddings(inputs) # shape is (seq, batch, emb)\n",
    "        lstm_out, _ = self.lstm(embeds) # (seq, batch, emb) -> (seq, batch, hidden_dim)\n",
    "#         lstm_out, _ = self.lstm(embeds.view(seq_len, batch_size, -1)) # why do we need view?\n",
    "#         lstm_out, self.hidden = self.lstm(embeds.view(seq_len, batch_size, -1), self.hidden)\n",
    "        # (seq, batch, hidden_dim) -> (seq * batch, hidden_dim)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(seq_len * batch_size, -1)) \n",
    "#         tag_space = self.hidden2tag(lstm_out.view(seq_len, -1))\n",
    "\n",
    "#         tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        # (seq * batch, hidden_dim) -> (seq, batch, tagset_size)\n",
    "#         tag_scores = tag_scores.view(seq_len, batch_size, self.tagset_size)\n",
    "        \n",
    "        tag_scores = tag_space.view(seq_len, batch_size, self.tagset_size)\n",
    "        \n",
    "        return tag_scores\n",
    "    \n",
    "#     def init_hidden(self, batch_size):\n",
    "#         hidden = Variable(next(self.parameters()).data.new(2 * self.num_layers, batch_size, self.hidden_dim // 2))\n",
    "#         cell = Variable(next(self.parameters()).data.new(2 * self.num_layers, batch_size, self.hidden_dim // 2))\n",
    "        \n",
    "#         return hidden.zero_(), cell.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=embeddings,\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(train_data, y_train), epochs_count=35,\n",
    "    batch_size=64, val_data=(val_data, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_epoch(model, criterion, data=(test_data, y_test), batch_size=512)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 06 - RNNs, part 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
