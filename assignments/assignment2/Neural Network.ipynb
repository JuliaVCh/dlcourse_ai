{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W_1\n",
      "Gradient check passed!\n",
      "Checking gradient for W_2\n",
      "Gradient check passed!\n",
      "Checking gradient for B_1\n",
      "Gradient check passed!\n",
      "Checking gradient for B_2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W_1\n",
      "Gradient check passed!\n",
      "Checking gradient for W_2\n",
      "Gradient check passed!\n",
      "Checking gradient for B_1\n",
      "Gradient check passed!\n",
      "Checking gradient for B_2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.367089, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300552, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300047, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302550, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301167, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.299994, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303344, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301279, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302135, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301086, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301520, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302751, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302785, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302207, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303652, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301038, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302401, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302365, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301744, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302029, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-3)\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a1b9be7e10>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a1b9c8b780>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.477455, Train accuracy: 0.223333, val accuracy: 0.227000\n",
      "Loss: 3.307441, Train accuracy: 0.217778, val accuracy: 0.221000\n",
      "Loss: 2.630796, Train accuracy: 0.209889, val accuracy: 0.216000\n",
      "Loss: 2.432976, Train accuracy: 0.211667, val accuracy: 0.218000\n",
      "Loss: 2.299323, Train accuracy: 0.207111, val accuracy: 0.214000\n",
      "Loss: 2.225376, Train accuracy: 0.209000, val accuracy: 0.214000\n",
      "Loss: 2.164636, Train accuracy: 0.212111, val accuracy: 0.219000\n",
      "Loss: 2.357278, Train accuracy: 0.212556, val accuracy: 0.218000\n",
      "Loss: 2.292182, Train accuracy: 0.215111, val accuracy: 0.219000\n",
      "Loss: 2.282084, Train accuracy: 0.216889, val accuracy: 0.220000\n",
      "Loss: 2.230655, Train accuracy: 0.214111, val accuracy: 0.218000\n",
      "Loss: 2.242413, Train accuracy: 0.220778, val accuracy: 0.224000\n",
      "Loss: 2.438257, Train accuracy: 0.223778, val accuracy: 0.225000\n",
      "Loss: 2.149861, Train accuracy: 0.218222, val accuracy: 0.221000\n",
      "Loss: 2.445436, Train accuracy: 0.225444, val accuracy: 0.235000\n",
      "Loss: 2.252863, Train accuracy: 0.222667, val accuracy: 0.232000\n",
      "Loss: 2.252901, Train accuracy: 0.226333, val accuracy: 0.229000\n",
      "Loss: 2.189693, Train accuracy: 0.227000, val accuracy: 0.229000\n",
      "Loss: 2.205202, Train accuracy: 0.229556, val accuracy: 0.230000\n",
      "Loss: 2.298984, Train accuracy: 0.230889, val accuracy: 0.234000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.372315, Train accuracy: 0.171556, val accuracy: 0.178000\n",
      "Loss: 6.866657, Train accuracy: 0.203000, val accuracy: 0.226000\n",
      "Loss: 6.518223, Train accuracy: 0.213222, val accuracy: 0.214000\n",
      "Loss: 5.989601, Train accuracy: 0.215444, val accuracy: 0.213000\n",
      "Loss: 5.694062, Train accuracy: 0.215889, val accuracy: 0.216000\n",
      "Loss: 5.609747, Train accuracy: 0.216556, val accuracy: 0.214000\n",
      "Loss: 5.183031, Train accuracy: 0.218000, val accuracy: 0.217000\n",
      "Loss: 4.928319, Train accuracy: 0.216333, val accuracy: 0.219000\n",
      "Loss: 4.728091, Train accuracy: 0.217111, val accuracy: 0.220000\n",
      "Loss: 4.544668, Train accuracy: 0.216000, val accuracy: 0.221000\n",
      "Loss: 4.390727, Train accuracy: 0.216556, val accuracy: 0.221000\n",
      "Loss: 4.285611, Train accuracy: 0.216444, val accuracy: 0.222000\n",
      "Loss: 3.981328, Train accuracy: 0.215778, val accuracy: 0.221000\n",
      "Loss: 3.947577, Train accuracy: 0.215556, val accuracy: 0.222000\n",
      "Loss: 3.832581, Train accuracy: 0.216222, val accuracy: 0.222000\n",
      "Loss: 3.643637, Train accuracy: 0.215889, val accuracy: 0.222000\n",
      "Loss: 3.592111, Train accuracy: 0.216222, val accuracy: 0.222000\n",
      "Loss: 3.413925, Train accuracy: 0.216222, val accuracy: 0.222000\n",
      "Loss: 3.343187, Train accuracy: 0.216111, val accuracy: 0.221000\n",
      "Loss: 3.384620, Train accuracy: 0.216000, val accuracy: 0.221000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "#trainer.compute_accuracy(val_X, val_y)\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.033130, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 6.492887, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 6.292600, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 5.835474, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 5.175267, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 5.114466, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 4.839036, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 4.332513, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 3.923934, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 3.793648, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 3.649526, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 3.388257, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 3.343885, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 3.432993, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 3.164447, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.728196, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 3.018786, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.541885, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.547665, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.346438, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.287144, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.169514, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.278400, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.123367, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.041872, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.927931, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.972177, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.835487, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.795930, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.678634, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.676116, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.579692, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.549415, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.583070, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.450561, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.526510, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.350646, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.439596, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.305822, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.382276, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.308670, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.249941, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.312271, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.218257, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.144731, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.141749, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.159989, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.102986, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.097532, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.080032, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.047424, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.048735, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.004209, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.060787, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.028605, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.000529, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.927114, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.921837, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.977793, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.915795, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.013161, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.902651, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.920236, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.951474, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.934158, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.854676, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.838586, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.813014, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.837355, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.948313, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.876472, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.850345, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.859625, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.893057, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.849389, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.915072, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.893764, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.887842, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.803508, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.907924, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.861503, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.791052, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.827613, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.817390, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.766725, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.804020, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.772947, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.776543, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.833912, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.879302, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.814899, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.825587, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.824063, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.776321, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.833327, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.806273, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.789664, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.887936, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.796242, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.815505, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.859795, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.774128, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.867717, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.738370, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.834622, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.786160, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.800077, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.876599, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.768651, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.757140, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.839760, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.766066, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.743387, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.786709, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.806791, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.819998, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.835207, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.798600, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.840851, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.850643, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.783746, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.879830, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.729087, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.835846, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.767195, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.761909, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.759718, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.714041, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.691944, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.867151, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.863245, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.821648, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.752296, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.762513, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.748121, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.794206, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.763842, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.788270, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.793778, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.715493, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.804586, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.802929, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.766411, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.810536, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.769756, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.836111, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.786064, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.816618, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.818078, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.827109, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.140303, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 5.999131, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 6.343686, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 4.388766, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 2.898324, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 2.063525, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 3.828397, Train accuracy: 0.933333, val accuracy: 0.133333\n",
      "Loss: 2.849780, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 2.852911, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 3.220178, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 7.172790, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 5.103165, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 5.097703, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 6.987226, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 4.642470, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 9.055120, Train accuracy: 0.466667, val accuracy: 0.133333\n",
      "Loss: 16.415385, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 99.023452, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 16.249545, Train accuracy: 0.666667, val accuracy: 0.133333\n",
      "Loss: 20.829999, Train accuracy: 0.666667, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach 1.0 training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00e-03 1.00e-04 9.99e-01]\n",
      " [1.00e-03 1.00e-05 9.99e-01]\n",
      " [1.00e-03 1.00e-06 9.99e-01]\n",
      " [1.00e-04 1.00e-04 9.99e-01]\n",
      " [1.00e-04 1.00e-05 9.99e-01]\n",
      " [1.00e-04 1.00e-06 9.99e-01]\n",
      " [1.00e-05 1.00e-04 9.99e-01]\n",
      " [1.00e-05 1.00e-05 9.99e-01]\n",
      " [1.00e-05 1.00e-06 9.99e-01]\n",
      " [1.00e-03 1.00e-04 9.00e-01]\n",
      " [1.00e-03 1.00e-05 9.00e-01]\n",
      " [1.00e-03 1.00e-06 9.00e-01]\n",
      " [1.00e-04 1.00e-04 9.00e-01]\n",
      " [1.00e-04 1.00e-05 9.00e-01]\n",
      " [1.00e-04 1.00e-06 9.00e-01]\n",
      " [1.00e-05 1.00e-04 9.00e-01]\n",
      " [1.00e-05 1.00e-05 9.00e-01]\n",
      " [1.00e-05 1.00e-06 9.00e-01]\n",
      " [1.00e-03 1.00e-04 8.80e-01]\n",
      " [1.00e-03 1.00e-05 8.80e-01]\n",
      " [1.00e-03 1.00e-06 8.80e-01]\n",
      " [1.00e-04 1.00e-04 8.80e-01]\n",
      " [1.00e-04 1.00e-05 8.80e-01]\n",
      " [1.00e-04 1.00e-06 8.80e-01]\n",
      " [1.00e-05 1.00e-04 8.80e-01]\n",
      " [1.00e-05 1.00e-05 8.80e-01]\n",
      " [1.00e-05 1.00e-06 8.80e-01]]\n",
      "Loss: 2.187675, Train accuracy: 0.237111, val accuracy: 0.232000\n",
      "Loss: 2.002908, Train accuracy: 0.296222, val accuracy: 0.295000\n",
      "Loss: 1.889504, Train accuracy: 0.352222, val accuracy: 0.360000\n",
      "Loss: 1.821131, Train accuracy: 0.436889, val accuracy: 0.429000\n",
      "Loss: 1.869613, Train accuracy: 0.498111, val accuracy: 0.493000\n",
      "Loss: 1.625593, Train accuracy: 0.547000, val accuracy: 0.536000\n",
      "Loss: 1.596463, Train accuracy: 0.594556, val accuracy: 0.565000\n",
      "Loss: 1.311887, Train accuracy: 0.619667, val accuracy: 0.615000\n",
      "Loss: 0.995470, Train accuracy: 0.638444, val accuracy: 0.610000\n",
      "Loss: 1.036347, Train accuracy: 0.651000, val accuracy: 0.623000\n",
      "Loss: 1.154150, Train accuracy: 0.670222, val accuracy: 0.646000\n",
      "Loss: 1.005023, Train accuracy: 0.681111, val accuracy: 0.652000\n",
      "Loss: 1.245094, Train accuracy: 0.697556, val accuracy: 0.680000\n",
      "Loss: 0.949505, Train accuracy: 0.703222, val accuracy: 0.684000\n",
      "Loss: 1.078763, Train accuracy: 0.709444, val accuracy: 0.683000\n",
      "Loss: 1.092139, Train accuracy: 0.714778, val accuracy: 0.698000\n",
      "Loss: 0.921300, Train accuracy: 0.729667, val accuracy: 0.700000\n",
      "Loss: 1.044141, Train accuracy: 0.738333, val accuracy: 0.705000\n",
      "Loss: 0.882231, Train accuracy: 0.746667, val accuracy: 0.705000\n",
      "Loss: 0.701726, Train accuracy: 0.750556, val accuracy: 0.704000\n",
      "Loss: 2.064025, Train accuracy: 0.228889, val accuracy: 0.238000\n",
      "Loss: 1.916496, Train accuracy: 0.260889, val accuracy: 0.276000\n",
      "Loss: 2.247881, Train accuracy: 0.331333, val accuracy: 0.343000\n",
      "Loss: 1.796020, Train accuracy: 0.412889, val accuracy: 0.427000\n",
      "Loss: 1.836424, Train accuracy: 0.485111, val accuracy: 0.483000\n",
      "Loss: 1.718891, Train accuracy: 0.530778, val accuracy: 0.520000\n",
      "Loss: 1.435848, Train accuracy: 0.586111, val accuracy: 0.563000\n",
      "Loss: 1.398284, Train accuracy: 0.608556, val accuracy: 0.588000\n",
      "Loss: 1.595377, Train accuracy: 0.638333, val accuracy: 0.624000\n",
      "Loss: 1.092566, Train accuracy: 0.653444, val accuracy: 0.637000\n",
      "Loss: 1.489911, Train accuracy: 0.667111, val accuracy: 0.652000\n",
      "Loss: 1.300638, Train accuracy: 0.676778, val accuracy: 0.666000\n",
      "Loss: 1.258799, Train accuracy: 0.695111, val accuracy: 0.677000\n",
      "Loss: 1.633449, Train accuracy: 0.706778, val accuracy: 0.678000\n",
      "Loss: 0.779604, Train accuracy: 0.716889, val accuracy: 0.693000\n",
      "Loss: 0.890154, Train accuracy: 0.723667, val accuracy: 0.691000\n",
      "Loss: 1.341225, Train accuracy: 0.722111, val accuracy: 0.699000\n",
      "Loss: 0.809465, Train accuracy: 0.738667, val accuracy: 0.708000\n",
      "Loss: 0.832527, Train accuracy: 0.742556, val accuracy: 0.712000\n",
      "Loss: 0.979031, Train accuracy: 0.746222, val accuracy: 0.703000\n",
      "Loss: 2.085293, Train accuracy: 0.228333, val accuracy: 0.233000\n",
      "Loss: 2.169670, Train accuracy: 0.276333, val accuracy: 0.274000\n",
      "Loss: 2.086011, Train accuracy: 0.357333, val accuracy: 0.375000\n",
      "Loss: 2.120664, Train accuracy: 0.437000, val accuracy: 0.445000\n",
      "Loss: 1.466757, Train accuracy: 0.499444, val accuracy: 0.499000\n",
      "Loss: 1.640229, Train accuracy: 0.555889, val accuracy: 0.544000\n",
      "Loss: 1.383416, Train accuracy: 0.591444, val accuracy: 0.573000\n",
      "Loss: 1.115261, Train accuracy: 0.615667, val accuracy: 0.595000\n",
      "Loss: 1.392914, Train accuracy: 0.632000, val accuracy: 0.602000\n",
      "Loss: 1.169789, Train accuracy: 0.650222, val accuracy: 0.631000\n",
      "Loss: 1.034554, Train accuracy: 0.670889, val accuracy: 0.650000\n",
      "Loss: 1.151545, Train accuracy: 0.683444, val accuracy: 0.657000\n",
      "Loss: 1.227278, Train accuracy: 0.693556, val accuracy: 0.672000\n",
      "Loss: 1.305526, Train accuracy: 0.706778, val accuracy: 0.689000\n",
      "Loss: 1.192865, Train accuracy: 0.715000, val accuracy: 0.690000\n",
      "Loss: 0.725279, Train accuracy: 0.721222, val accuracy: 0.705000\n",
      "Loss: 1.225717, Train accuracy: 0.727667, val accuracy: 0.703000\n",
      "Loss: 1.095574, Train accuracy: 0.739222, val accuracy: 0.708000\n",
      "Loss: 1.061763, Train accuracy: 0.740667, val accuracy: 0.704000\n",
      "Loss: 1.121219, Train accuracy: 0.747222, val accuracy: 0.713000\n",
      "Loss: 2.282153, Train accuracy: 0.178222, val accuracy: 0.177000\n",
      "Loss: 2.206164, Train accuracy: 0.195111, val accuracy: 0.201000\n",
      "Loss: 2.268444, Train accuracy: 0.200889, val accuracy: 0.203000\n",
      "Loss: 2.255203, Train accuracy: 0.202889, val accuracy: 0.206000\n",
      "Loss: 2.159084, Train accuracy: 0.206556, val accuracy: 0.210000\n",
      "Loss: 2.137996, Train accuracy: 0.210333, val accuracy: 0.213000\n",
      "Loss: 2.268221, Train accuracy: 0.213667, val accuracy: 0.218000\n",
      "Loss: 2.112733, Train accuracy: 0.216444, val accuracy: 0.220000\n",
      "Loss: 2.273445, Train accuracy: 0.220556, val accuracy: 0.226000\n",
      "Loss: 2.206339, Train accuracy: 0.224333, val accuracy: 0.230000\n",
      "Loss: 2.095010, Train accuracy: 0.229222, val accuracy: 0.234000\n",
      "Loss: 2.108132, Train accuracy: 0.231889, val accuracy: 0.241000\n",
      "Loss: 2.159697, Train accuracy: 0.236667, val accuracy: 0.242000\n",
      "Loss: 2.110855, Train accuracy: 0.243333, val accuracy: 0.246000\n",
      "Loss: 2.039487, Train accuracy: 0.251222, val accuracy: 0.250000\n",
      "Loss: 2.041348, Train accuracy: 0.257444, val accuracy: 0.265000\n",
      "Loss: 2.132091, Train accuracy: 0.264222, val accuracy: 0.277000\n",
      "Loss: 2.193033, Train accuracy: 0.270667, val accuracy: 0.286000\n",
      "Loss: 2.239553, Train accuracy: 0.278444, val accuracy: 0.296000\n",
      "Loss: 2.051242, Train accuracy: 0.287667, val accuracy: 0.302000\n",
      "Loss: 2.285950, Train accuracy: 0.181556, val accuracy: 0.178000\n",
      "Loss: 2.277518, Train accuracy: 0.200222, val accuracy: 0.209000\n",
      "Loss: 2.342614, Train accuracy: 0.205444, val accuracy: 0.216000\n",
      "Loss: 2.071533, Train accuracy: 0.212889, val accuracy: 0.224000\n",
      "Loss: 2.077137, Train accuracy: 0.214889, val accuracy: 0.219000\n",
      "Loss: 2.229884, Train accuracy: 0.220333, val accuracy: 0.224000\n",
      "Loss: 2.237579, Train accuracy: 0.220667, val accuracy: 0.226000\n",
      "Loss: 2.185788, Train accuracy: 0.224111, val accuracy: 0.228000\n",
      "Loss: 2.139989, Train accuracy: 0.228000, val accuracy: 0.234000\n",
      "Loss: 2.240773, Train accuracy: 0.232778, val accuracy: 0.239000\n",
      "Loss: 2.130731, Train accuracy: 0.237333, val accuracy: 0.243000\n",
      "Loss: 2.324974, Train accuracy: 0.244444, val accuracy: 0.249000\n",
      "Loss: 2.018833, Train accuracy: 0.251556, val accuracy: 0.258000\n",
      "Loss: 2.035645, Train accuracy: 0.255889, val accuracy: 0.261000\n",
      "Loss: 2.142323, Train accuracy: 0.264889, val accuracy: 0.266000\n",
      "Loss: 1.972174, Train accuracy: 0.271111, val accuracy: 0.277000\n",
      "Loss: 2.310354, Train accuracy: 0.279222, val accuracy: 0.287000\n",
      "Loss: 2.092109, Train accuracy: 0.285222, val accuracy: 0.296000\n",
      "Loss: 1.789186, Train accuracy: 0.294889, val accuracy: 0.303000\n",
      "Loss: 2.119283, Train accuracy: 0.303444, val accuracy: 0.317000\n",
      "Loss: 2.206356, Train accuracy: 0.172778, val accuracy: 0.173000\n",
      "Loss: 2.274797, Train accuracy: 0.194333, val accuracy: 0.197000\n",
      "Loss: 2.257532, Train accuracy: 0.201333, val accuracy: 0.209000\n",
      "Loss: 2.203598, Train accuracy: 0.205444, val accuracy: 0.210000\n",
      "Loss: 2.209668, Train accuracy: 0.209111, val accuracy: 0.216000\n",
      "Loss: 2.174302, Train accuracy: 0.213889, val accuracy: 0.221000\n",
      "Loss: 2.324610, Train accuracy: 0.220333, val accuracy: 0.227000\n",
      "Loss: 2.200677, Train accuracy: 0.222889, val accuracy: 0.228000\n",
      "Loss: 2.227580, Train accuracy: 0.227000, val accuracy: 0.236000\n",
      "Loss: 2.118663, Train accuracy: 0.230889, val accuracy: 0.241000\n",
      "Loss: 2.102421, Train accuracy: 0.236000, val accuracy: 0.247000\n",
      "Loss: 2.109971, Train accuracy: 0.239000, val accuracy: 0.245000\n",
      "Loss: 2.135545, Train accuracy: 0.246222, val accuracy: 0.255000\n",
      "Loss: 2.146193, Train accuracy: 0.249222, val accuracy: 0.256000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.164196, Train accuracy: 0.257667, val accuracy: 0.260000\n",
      "Loss: 2.247411, Train accuracy: 0.264111, val accuracy: 0.271000\n",
      "Loss: 1.947980, Train accuracy: 0.271778, val accuracy: 0.279000\n",
      "Loss: 1.976874, Train accuracy: 0.281000, val accuracy: 0.287000\n",
      "Loss: 1.969848, Train accuracy: 0.288889, val accuracy: 0.294000\n",
      "Loss: 2.056011, Train accuracy: 0.293778, val accuracy: 0.297000\n",
      "Loss: 2.241863, Train accuracy: 0.183556, val accuracy: 0.196000\n",
      "Loss: 2.337257, Train accuracy: 0.186556, val accuracy: 0.197000\n",
      "Loss: 2.374602, Train accuracy: 0.191000, val accuracy: 0.201000\n",
      "Loss: 2.336658, Train accuracy: 0.195111, val accuracy: 0.200000\n",
      "Loss: 2.291668, Train accuracy: 0.195778, val accuracy: 0.206000\n",
      "Loss: 2.262613, Train accuracy: 0.196222, val accuracy: 0.203000\n",
      "Loss: 2.291003, Train accuracy: 0.194889, val accuracy: 0.205000\n",
      "Loss: 2.230081, Train accuracy: 0.195889, val accuracy: 0.204000\n",
      "Loss: 2.251647, Train accuracy: 0.196000, val accuracy: 0.203000\n",
      "Loss: 2.271598, Train accuracy: 0.196222, val accuracy: 0.202000\n",
      "Loss: 2.294867, Train accuracy: 0.196222, val accuracy: 0.204000\n",
      "Loss: 2.143512, Train accuracy: 0.197222, val accuracy: 0.203000\n",
      "Loss: 2.175619, Train accuracy: 0.197778, val accuracy: 0.204000\n",
      "Loss: 2.173450, Train accuracy: 0.198333, val accuracy: 0.204000\n",
      "Loss: 2.313835, Train accuracy: 0.198333, val accuracy: 0.203000\n",
      "Loss: 2.106271, Train accuracy: 0.198556, val accuracy: 0.204000\n",
      "Loss: 2.182702, Train accuracy: 0.198222, val accuracy: 0.204000\n",
      "Loss: 2.274246, Train accuracy: 0.198556, val accuracy: 0.204000\n",
      "Loss: 2.380044, Train accuracy: 0.198556, val accuracy: 0.204000\n",
      "Loss: 2.221833, Train accuracy: 0.198444, val accuracy: 0.205000\n",
      "Loss: 2.315184, Train accuracy: 0.090444, val accuracy: 0.090000\n",
      "Loss: 2.339555, Train accuracy: 0.099556, val accuracy: 0.100000\n",
      "Loss: 2.277326, Train accuracy: 0.108111, val accuracy: 0.104000\n",
      "Loss: 2.301881, Train accuracy: 0.119111, val accuracy: 0.117000\n",
      "Loss: 2.313438, Train accuracy: 0.130111, val accuracy: 0.123000\n",
      "Loss: 2.318186, Train accuracy: 0.141333, val accuracy: 0.138000\n",
      "Loss: 2.265274, Train accuracy: 0.148667, val accuracy: 0.144000\n",
      "Loss: 2.283085, Train accuracy: 0.155778, val accuracy: 0.153000\n",
      "Loss: 2.271482, Train accuracy: 0.163889, val accuracy: 0.159000\n",
      "Loss: 2.263354, Train accuracy: 0.167222, val accuracy: 0.165000\n",
      "Loss: 2.289901, Train accuracy: 0.171778, val accuracy: 0.169000\n",
      "Loss: 2.317069, Train accuracy: 0.176667, val accuracy: 0.170000\n",
      "Loss: 2.217197, Train accuracy: 0.181556, val accuracy: 0.175000\n",
      "Loss: 2.298793, Train accuracy: 0.183556, val accuracy: 0.178000\n",
      "Loss: 2.253208, Train accuracy: 0.184778, val accuracy: 0.178000\n",
      "Loss: 2.224570, Train accuracy: 0.186333, val accuracy: 0.183000\n",
      "Loss: 2.248931, Train accuracy: 0.189000, val accuracy: 0.180000\n",
      "Loss: 2.285821, Train accuracy: 0.189444, val accuracy: 0.183000\n",
      "Loss: 2.278544, Train accuracy: 0.191444, val accuracy: 0.183000\n",
      "Loss: 2.311649, Train accuracy: 0.193333, val accuracy: 0.188000\n",
      "Loss: 2.300652, Train accuracy: 0.106889, val accuracy: 0.131000\n",
      "Loss: 2.278333, Train accuracy: 0.112889, val accuracy: 0.138000\n",
      "Loss: 2.340214, Train accuracy: 0.117778, val accuracy: 0.148000\n",
      "Loss: 2.292846, Train accuracy: 0.125667, val accuracy: 0.153000\n",
      "Loss: 2.302217, Train accuracy: 0.132778, val accuracy: 0.151000\n",
      "Loss: 2.316718, Train accuracy: 0.139444, val accuracy: 0.154000\n",
      "Loss: 2.306899, Train accuracy: 0.144000, val accuracy: 0.155000\n",
      "Loss: 2.273342, Train accuracy: 0.152222, val accuracy: 0.164000\n",
      "Loss: 2.352514, Train accuracy: 0.161556, val accuracy: 0.172000\n",
      "Loss: 2.222375, Train accuracy: 0.168889, val accuracy: 0.183000\n",
      "Loss: 2.257001, Train accuracy: 0.174889, val accuracy: 0.180000\n",
      "Loss: 2.231413, Train accuracy: 0.179444, val accuracy: 0.183000\n",
      "Loss: 2.323397, Train accuracy: 0.186111, val accuracy: 0.189000\n",
      "Loss: 2.203348, Train accuracy: 0.188778, val accuracy: 0.187000\n",
      "Loss: 2.200092, Train accuracy: 0.187000, val accuracy: 0.187000\n",
      "Loss: 2.300706, Train accuracy: 0.189556, val accuracy: 0.189000\n",
      "Loss: 2.269215, Train accuracy: 0.191000, val accuracy: 0.184000\n",
      "Loss: 2.190939, Train accuracy: 0.194333, val accuracy: 0.186000\n",
      "Loss: 2.218767, Train accuracy: 0.195556, val accuracy: 0.193000\n",
      "Loss: 2.234045, Train accuracy: 0.199556, val accuracy: 0.197000\n",
      "Loss: 2.110520, Train accuracy: 0.244333, val accuracy: 0.247000\n",
      "Loss: 2.020768, Train accuracy: 0.273556, val accuracy: 0.274000\n",
      "Loss: 1.860657, Train accuracy: 0.330333, val accuracy: 0.330000\n",
      "Loss: 1.821182, Train accuracy: 0.388000, val accuracy: 0.390000\n",
      "Loss: 2.005803, Train accuracy: 0.425889, val accuracy: 0.430000\n",
      "Loss: 1.886278, Train accuracy: 0.468444, val accuracy: 0.470000\n",
      "Loss: 1.929129, Train accuracy: 0.510000, val accuracy: 0.504000\n",
      "Loss: 1.458709, Train accuracy: 0.537556, val accuracy: 0.524000\n",
      "Loss: 1.720658, Train accuracy: 0.559667, val accuracy: 0.546000\n",
      "Loss: 1.543725, Train accuracy: 0.570333, val accuracy: 0.547000\n",
      "Loss: 1.256674, Train accuracy: 0.583333, val accuracy: 0.564000\n",
      "Loss: 1.423693, Train accuracy: 0.597222, val accuracy: 0.584000\n",
      "Loss: 1.500038, Train accuracy: 0.605667, val accuracy: 0.588000\n",
      "Loss: 1.458066, Train accuracy: 0.614000, val accuracy: 0.596000\n",
      "Loss: 1.444908, Train accuracy: 0.622222, val accuracy: 0.602000\n",
      "Loss: 1.400935, Train accuracy: 0.624333, val accuracy: 0.606000\n",
      "Loss: 1.567107, Train accuracy: 0.628444, val accuracy: 0.607000\n",
      "Loss: 1.385634, Train accuracy: 0.635222, val accuracy: 0.615000\n",
      "Loss: 1.084658, Train accuracy: 0.639667, val accuracy: 0.612000\n",
      "Loss: 1.487669, Train accuracy: 0.641222, val accuracy: 0.613000\n",
      "Loss: 2.215295, Train accuracy: 0.228667, val accuracy: 0.230000\n",
      "Loss: 2.132176, Train accuracy: 0.272000, val accuracy: 0.274000\n",
      "Loss: 1.939376, Train accuracy: 0.334111, val accuracy: 0.347000\n",
      "Loss: 1.656610, Train accuracy: 0.404556, val accuracy: 0.405000\n",
      "Loss: 1.666169, Train accuracy: 0.449778, val accuracy: 0.454000\n",
      "Loss: 1.713738, Train accuracy: 0.491444, val accuracy: 0.485000\n",
      "Loss: 1.418077, Train accuracy: 0.530667, val accuracy: 0.510000\n",
      "Loss: 1.418711, Train accuracy: 0.555000, val accuracy: 0.541000\n",
      "Loss: 1.604644, Train accuracy: 0.574778, val accuracy: 0.564000\n",
      "Loss: 1.187670, Train accuracy: 0.592000, val accuracy: 0.568000\n",
      "Loss: 1.055009, Train accuracy: 0.599889, val accuracy: 0.581000\n",
      "Loss: 1.266180, Train accuracy: 0.613778, val accuracy: 0.591000\n",
      "Loss: 1.663286, Train accuracy: 0.624000, val accuracy: 0.608000\n",
      "Loss: 1.340377, Train accuracy: 0.630333, val accuracy: 0.615000\n",
      "Loss: 1.101686, Train accuracy: 0.635000, val accuracy: 0.621000\n",
      "Loss: 1.444196, Train accuracy: 0.642111, val accuracy: 0.618000\n",
      "Loss: 1.083525, Train accuracy: 0.642889, val accuracy: 0.625000\n",
      "Loss: 1.299926, Train accuracy: 0.645444, val accuracy: 0.629000\n",
      "Loss: 1.410374, Train accuracy: 0.648222, val accuracy: 0.629000\n",
      "Loss: 0.918264, Train accuracy: 0.651778, val accuracy: 0.634000\n",
      "Loss: 2.362761, Train accuracy: 0.223333, val accuracy: 0.232000\n",
      "Loss: 2.148482, Train accuracy: 0.278667, val accuracy: 0.284000\n",
      "Loss: 2.020536, Train accuracy: 0.352778, val accuracy: 0.368000\n",
      "Loss: 1.804912, Train accuracy: 0.401778, val accuracy: 0.412000\n",
      "Loss: 1.916273, Train accuracy: 0.450222, val accuracy: 0.456000\n",
      "Loss: 1.509932, Train accuracy: 0.496333, val accuracy: 0.495000\n",
      "Loss: 1.690499, Train accuracy: 0.524333, val accuracy: 0.514000\n",
      "Loss: 1.731397, Train accuracy: 0.552000, val accuracy: 0.539000\n",
      "Loss: 1.530020, Train accuracy: 0.571222, val accuracy: 0.551000\n",
      "Loss: 1.234563, Train accuracy: 0.591000, val accuracy: 0.575000\n",
      "Loss: 1.602747, Train accuracy: 0.602889, val accuracy: 0.585000\n",
      "Loss: 1.405251, Train accuracy: 0.612889, val accuracy: 0.592000\n",
      "Loss: 1.313637, Train accuracy: 0.622222, val accuracy: 0.606000\n",
      "Loss: 1.014977, Train accuracy: 0.628111, val accuracy: 0.606000\n",
      "Loss: 1.276610, Train accuracy: 0.638333, val accuracy: 0.612000\n",
      "Loss: 1.354282, Train accuracy: 0.639556, val accuracy: 0.623000\n",
      "Loss: 1.538783, Train accuracy: 0.644444, val accuracy: 0.624000\n",
      "Loss: 1.465729, Train accuracy: 0.649000, val accuracy: 0.626000\n",
      "Loss: 0.954071, Train accuracy: 0.651667, val accuracy: 0.629000\n",
      "Loss: 1.155042, Train accuracy: 0.654111, val accuracy: 0.633000\n",
      "Loss: 2.307492, Train accuracy: 0.162889, val accuracy: 0.148000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.287567, Train accuracy: 0.197778, val accuracy: 0.187000\n",
      "Loss: 2.214853, Train accuracy: 0.209333, val accuracy: 0.203000\n",
      "Loss: 2.274580, Train accuracy: 0.215222, val accuracy: 0.212000\n",
      "Loss: 2.322968, Train accuracy: 0.220000, val accuracy: 0.221000\n",
      "Loss: 2.148816, Train accuracy: 0.223667, val accuracy: 0.225000\n",
      "Loss: 2.263555, Train accuracy: 0.224778, val accuracy: 0.226000\n",
      "Loss: 2.178193, Train accuracy: 0.227889, val accuracy: 0.229000\n",
      "Loss: 2.134751, Train accuracy: 0.229556, val accuracy: 0.232000\n",
      "Loss: 2.133743, Train accuracy: 0.229000, val accuracy: 0.230000\n",
      "Loss: 2.204430, Train accuracy: 0.229889, val accuracy: 0.236000\n",
      "Loss: 2.257819, Train accuracy: 0.230889, val accuracy: 0.237000\n",
      "Loss: 2.110376, Train accuracy: 0.233778, val accuracy: 0.237000\n",
      "Loss: 2.302423, Train accuracy: 0.234333, val accuracy: 0.237000\n",
      "Loss: 2.096431, Train accuracy: 0.235222, val accuracy: 0.238000\n",
      "Loss: 2.125552, Train accuracy: 0.235222, val accuracy: 0.239000\n",
      "Loss: 2.190429, Train accuracy: 0.235889, val accuracy: 0.240000\n",
      "Loss: 2.022732, Train accuracy: 0.235667, val accuracy: 0.241000\n",
      "Loss: 2.311762, Train accuracy: 0.236222, val accuracy: 0.241000\n",
      "Loss: 2.285221, Train accuracy: 0.236556, val accuracy: 0.242000\n",
      "Loss: 2.308620, Train accuracy: 0.189000, val accuracy: 0.197000\n",
      "Loss: 2.225819, Train accuracy: 0.199222, val accuracy: 0.206000\n",
      "Loss: 2.227761, Train accuracy: 0.204000, val accuracy: 0.210000\n",
      "Loss: 2.351284, Train accuracy: 0.206444, val accuracy: 0.211000\n",
      "Loss: 2.251801, Train accuracy: 0.210111, val accuracy: 0.218000\n",
      "Loss: 2.173853, Train accuracy: 0.214444, val accuracy: 0.223000\n",
      "Loss: 2.208178, Train accuracy: 0.217222, val accuracy: 0.224000\n",
      "Loss: 2.185085, Train accuracy: 0.219444, val accuracy: 0.225000\n",
      "Loss: 2.230676, Train accuracy: 0.221000, val accuracy: 0.227000\n",
      "Loss: 2.139233, Train accuracy: 0.222556, val accuracy: 0.228000\n",
      "Loss: 2.287112, Train accuracy: 0.223556, val accuracy: 0.231000\n",
      "Loss: 2.203081, Train accuracy: 0.224889, val accuracy: 0.232000\n",
      "Loss: 2.200968, Train accuracy: 0.226111, val accuracy: 0.232000\n",
      "Loss: 2.102183, Train accuracy: 0.226778, val accuracy: 0.234000\n",
      "Loss: 2.278142, Train accuracy: 0.227778, val accuracy: 0.235000\n",
      "Loss: 2.259965, Train accuracy: 0.228889, val accuracy: 0.238000\n",
      "Loss: 2.173383, Train accuracy: 0.229444, val accuracy: 0.238000\n",
      "Loss: 2.178191, Train accuracy: 0.230222, val accuracy: 0.238000\n",
      "Loss: 2.125124, Train accuracy: 0.231222, val accuracy: 0.240000\n",
      "Loss: 2.042179, Train accuracy: 0.232667, val accuracy: 0.240000\n",
      "Loss: 2.270843, Train accuracy: 0.160333, val accuracy: 0.171000\n",
      "Loss: 2.119292, Train accuracy: 0.187000, val accuracy: 0.200000\n",
      "Loss: 2.203820, Train accuracy: 0.200889, val accuracy: 0.218000\n",
      "Loss: 2.223811, Train accuracy: 0.209889, val accuracy: 0.233000\n",
      "Loss: 2.216194, Train accuracy: 0.218556, val accuracy: 0.242000\n",
      "Loss: 2.164056, Train accuracy: 0.221667, val accuracy: 0.243000\n",
      "Loss: 2.247746, Train accuracy: 0.224778, val accuracy: 0.244000\n",
      "Loss: 2.131556, Train accuracy: 0.227889, val accuracy: 0.240000\n",
      "Loss: 2.325725, Train accuracy: 0.229889, val accuracy: 0.238000\n",
      "Loss: 2.253007, Train accuracy: 0.232222, val accuracy: 0.242000\n",
      "Loss: 2.048704, Train accuracy: 0.232778, val accuracy: 0.242000\n",
      "Loss: 2.072159, Train accuracy: 0.233556, val accuracy: 0.242000\n",
      "Loss: 2.284831, Train accuracy: 0.234444, val accuracy: 0.242000\n",
      "Loss: 2.200463, Train accuracy: 0.235222, val accuracy: 0.244000\n",
      "Loss: 2.256706, Train accuracy: 0.235667, val accuracy: 0.244000\n",
      "Loss: 2.121734, Train accuracy: 0.236444, val accuracy: 0.245000\n",
      "Loss: 2.054032, Train accuracy: 0.237111, val accuracy: 0.244000\n",
      "Loss: 2.202864, Train accuracy: 0.238111, val accuracy: 0.245000\n",
      "Loss: 2.101848, Train accuracy: 0.238444, val accuracy: 0.246000\n",
      "Loss: 2.344010, Train accuracy: 0.239111, val accuracy: 0.246000\n",
      "Loss: 2.345469, Train accuracy: 0.102889, val accuracy: 0.106000\n",
      "Loss: 2.310413, Train accuracy: 0.111111, val accuracy: 0.114000\n",
      "Loss: 2.262113, Train accuracy: 0.117667, val accuracy: 0.116000\n",
      "Loss: 2.287149, Train accuracy: 0.124667, val accuracy: 0.123000\n",
      "Loss: 2.253968, Train accuracy: 0.128444, val accuracy: 0.125000\n",
      "Loss: 2.328920, Train accuracy: 0.132556, val accuracy: 0.133000\n",
      "Loss: 2.278043, Train accuracy: 0.137111, val accuracy: 0.135000\n",
      "Loss: 2.302999, Train accuracy: 0.139333, val accuracy: 0.134000\n",
      "Loss: 2.202351, Train accuracy: 0.141333, val accuracy: 0.135000\n",
      "Loss: 2.308973, Train accuracy: 0.143778, val accuracy: 0.140000\n",
      "Loss: 2.265859, Train accuracy: 0.147111, val accuracy: 0.144000\n",
      "Loss: 2.246883, Train accuracy: 0.149111, val accuracy: 0.147000\n",
      "Loss: 2.254663, Train accuracy: 0.151000, val accuracy: 0.154000\n",
      "Loss: 2.287900, Train accuracy: 0.153667, val accuracy: 0.159000\n",
      "Loss: 2.250534, Train accuracy: 0.154333, val accuracy: 0.161000\n",
      "Loss: 2.248178, Train accuracy: 0.157000, val accuracy: 0.164000\n",
      "Loss: 2.286419, Train accuracy: 0.158778, val accuracy: 0.169000\n",
      "Loss: 2.288812, Train accuracy: 0.160667, val accuracy: 0.169000\n",
      "Loss: 2.249370, Train accuracy: 0.161222, val accuracy: 0.167000\n",
      "Loss: 2.276352, Train accuracy: 0.162889, val accuracy: 0.169000\n",
      "Loss: 2.306100, Train accuracy: 0.159889, val accuracy: 0.177000\n",
      "Loss: 2.247877, Train accuracy: 0.169889, val accuracy: 0.187000\n",
      "Loss: 2.308534, Train accuracy: 0.177667, val accuracy: 0.193000\n",
      "Loss: 2.325443, Train accuracy: 0.181889, val accuracy: 0.192000\n",
      "Loss: 2.253586, Train accuracy: 0.183222, val accuracy: 0.196000\n",
      "Loss: 2.296133, Train accuracy: 0.185778, val accuracy: 0.195000\n",
      "Loss: 2.286511, Train accuracy: 0.186556, val accuracy: 0.200000\n",
      "Loss: 2.277765, Train accuracy: 0.188556, val accuracy: 0.201000\n",
      "Loss: 2.263636, Train accuracy: 0.190556, val accuracy: 0.203000\n",
      "Loss: 2.326422, Train accuracy: 0.191889, val accuracy: 0.205000\n",
      "Loss: 2.272583, Train accuracy: 0.192222, val accuracy: 0.206000\n",
      "Loss: 2.300873, Train accuracy: 0.193333, val accuracy: 0.205000\n",
      "Loss: 2.283851, Train accuracy: 0.194111, val accuracy: 0.206000\n",
      "Loss: 2.198761, Train accuracy: 0.194778, val accuracy: 0.205000\n",
      "Loss: 2.230917, Train accuracy: 0.195000, val accuracy: 0.204000\n",
      "Loss: 2.219839, Train accuracy: 0.195556, val accuracy: 0.204000\n",
      "Loss: 2.252087, Train accuracy: 0.195444, val accuracy: 0.204000\n",
      "Loss: 2.275579, Train accuracy: 0.195667, val accuracy: 0.204000\n",
      "Loss: 2.302602, Train accuracy: 0.195444, val accuracy: 0.205000\n",
      "Loss: 2.344134, Train accuracy: 0.195889, val accuracy: 0.205000\n",
      "Loss: 2.351651, Train accuracy: 0.088111, val accuracy: 0.085000\n",
      "Loss: 2.289271, Train accuracy: 0.102778, val accuracy: 0.093000\n",
      "Loss: 2.291712, Train accuracy: 0.114889, val accuracy: 0.112000\n",
      "Loss: 2.301423, Train accuracy: 0.123556, val accuracy: 0.127000\n",
      "Loss: 2.293555, Train accuracy: 0.130667, val accuracy: 0.135000\n",
      "Loss: 2.273144, Train accuracy: 0.138444, val accuracy: 0.136000\n",
      "Loss: 2.274601, Train accuracy: 0.143444, val accuracy: 0.141000\n",
      "Loss: 2.300712, Train accuracy: 0.149000, val accuracy: 0.149000\n",
      "Loss: 2.261415, Train accuracy: 0.153444, val accuracy: 0.161000\n",
      "Loss: 2.288049, Train accuracy: 0.157000, val accuracy: 0.167000\n",
      "Loss: 2.345319, Train accuracy: 0.160667, val accuracy: 0.169000\n",
      "Loss: 2.283876, Train accuracy: 0.164667, val accuracy: 0.173000\n",
      "Loss: 2.260428, Train accuracy: 0.168000, val accuracy: 0.176000\n",
      "Loss: 2.256576, Train accuracy: 0.169556, val accuracy: 0.178000\n",
      "Loss: 2.227700, Train accuracy: 0.170111, val accuracy: 0.180000\n",
      "Loss: 2.243350, Train accuracy: 0.171333, val accuracy: 0.178000\n",
      "Loss: 2.308675, Train accuracy: 0.173333, val accuracy: 0.182000\n",
      "Loss: 2.203417, Train accuracy: 0.173556, val accuracy: 0.183000\n",
      "Loss: 2.245640, Train accuracy: 0.174111, val accuracy: 0.183000\n",
      "Loss: 2.262185, Train accuracy: 0.174778, val accuracy: 0.184000\n",
      "Loss: 2.094414, Train accuracy: 0.229222, val accuracy: 0.236000\n",
      "Loss: 2.087447, Train accuracy: 0.271667, val accuracy: 0.282000\n",
      "Loss: 2.153429, Train accuracy: 0.320444, val accuracy: 0.336000\n",
      "Loss: 1.474792, Train accuracy: 0.379556, val accuracy: 0.391000\n",
      "Loss: 1.995613, Train accuracy: 0.430000, val accuracy: 0.442000\n",
      "Loss: 1.567373, Train accuracy: 0.467333, val accuracy: 0.487000\n",
      "Loss: 1.526490, Train accuracy: 0.509444, val accuracy: 0.511000\n",
      "Loss: 1.448442, Train accuracy: 0.532000, val accuracy: 0.538000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.455212, Train accuracy: 0.548222, val accuracy: 0.555000\n",
      "Loss: 1.568254, Train accuracy: 0.564111, val accuracy: 0.568000\n",
      "Loss: 1.496244, Train accuracy: 0.573778, val accuracy: 0.577000\n",
      "Loss: 1.456174, Train accuracy: 0.587222, val accuracy: 0.587000\n",
      "Loss: 1.391031, Train accuracy: 0.597778, val accuracy: 0.589000\n",
      "Loss: 1.699301, Train accuracy: 0.605111, val accuracy: 0.595000\n",
      "Loss: 1.224461, Train accuracy: 0.607556, val accuracy: 0.600000\n",
      "Loss: 1.932891, Train accuracy: 0.614778, val accuracy: 0.603000\n",
      "Loss: 1.143640, Train accuracy: 0.617778, val accuracy: 0.601000\n",
      "Loss: 1.316864, Train accuracy: 0.621000, val accuracy: 0.602000\n",
      "Loss: 1.371951, Train accuracy: 0.623667, val accuracy: 0.608000\n",
      "Loss: 1.595970, Train accuracy: 0.624667, val accuracy: 0.607000\n",
      "Loss: 2.260912, Train accuracy: 0.226111, val accuracy: 0.228000\n",
      "Loss: 2.047403, Train accuracy: 0.270778, val accuracy: 0.284000\n",
      "Loss: 1.604927, Train accuracy: 0.337333, val accuracy: 0.351000\n",
      "Loss: 1.727664, Train accuracy: 0.392333, val accuracy: 0.400000\n",
      "Loss: 1.553388, Train accuracy: 0.438889, val accuracy: 0.444000\n",
      "Loss: 1.631247, Train accuracy: 0.477444, val accuracy: 0.484000\n",
      "Loss: 1.655825, Train accuracy: 0.499000, val accuracy: 0.495000\n",
      "Loss: 1.626474, Train accuracy: 0.535778, val accuracy: 0.529000\n",
      "Loss: 1.510178, Train accuracy: 0.546667, val accuracy: 0.534000\n",
      "Loss: 1.396952, Train accuracy: 0.563444, val accuracy: 0.550000\n",
      "Loss: 1.484524, Train accuracy: 0.572444, val accuracy: 0.553000\n",
      "Loss: 1.277928, Train accuracy: 0.585333, val accuracy: 0.567000\n",
      "Loss: 1.331332, Train accuracy: 0.594222, val accuracy: 0.579000\n",
      "Loss: 1.704836, Train accuracy: 0.597778, val accuracy: 0.585000\n",
      "Loss: 1.372741, Train accuracy: 0.605333, val accuracy: 0.584000\n",
      "Loss: 1.198904, Train accuracy: 0.610000, val accuracy: 0.586000\n",
      "Loss: 1.260927, Train accuracy: 0.612222, val accuracy: 0.597000\n",
      "Loss: 1.689255, Train accuracy: 0.616556, val accuracy: 0.599000\n",
      "Loss: 1.321098, Train accuracy: 0.618111, val accuracy: 0.599000\n",
      "Loss: 1.526257, Train accuracy: 0.621111, val accuracy: 0.604000\n",
      "Loss: 2.122180, Train accuracy: 0.240444, val accuracy: 0.247000\n",
      "Loss: 1.838303, Train accuracy: 0.271444, val accuracy: 0.269000\n",
      "Loss: 2.123454, Train accuracy: 0.324889, val accuracy: 0.331000\n",
      "Loss: 2.080025, Train accuracy: 0.376000, val accuracy: 0.389000\n",
      "Loss: 1.825707, Train accuracy: 0.415556, val accuracy: 0.417000\n",
      "Loss: 1.651068, Train accuracy: 0.459444, val accuracy: 0.469000\n",
      "Loss: 2.008839, Train accuracy: 0.486000, val accuracy: 0.486000\n",
      "Loss: 1.765754, Train accuracy: 0.509111, val accuracy: 0.505000\n",
      "Loss: 1.554457, Train accuracy: 0.532333, val accuracy: 0.514000\n",
      "Loss: 1.489623, Train accuracy: 0.557222, val accuracy: 0.539000\n",
      "Loss: 1.292478, Train accuracy: 0.571889, val accuracy: 0.551000\n",
      "Loss: 1.423570, Train accuracy: 0.580222, val accuracy: 0.561000\n",
      "Loss: 1.512932, Train accuracy: 0.592333, val accuracy: 0.573000\n",
      "Loss: 1.358901, Train accuracy: 0.599222, val accuracy: 0.581000\n",
      "Loss: 1.462644, Train accuracy: 0.604222, val accuracy: 0.587000\n",
      "Loss: 1.528075, Train accuracy: 0.610333, val accuracy: 0.589000\n",
      "Loss: 1.530052, Train accuracy: 0.613111, val accuracy: 0.588000\n",
      "Loss: 1.264900, Train accuracy: 0.615667, val accuracy: 0.592000\n",
      "Loss: 1.431880, Train accuracy: 0.619222, val accuracy: 0.596000\n",
      "Loss: 1.342228, Train accuracy: 0.621444, val accuracy: 0.599000\n",
      "Loss: 2.262426, Train accuracy: 0.188889, val accuracy: 0.198000\n",
      "Loss: 2.235228, Train accuracy: 0.197667, val accuracy: 0.205000\n",
      "Loss: 2.378669, Train accuracy: 0.201333, val accuracy: 0.207000\n",
      "Loss: 2.234244, Train accuracy: 0.202667, val accuracy: 0.208000\n",
      "Loss: 2.227675, Train accuracy: 0.203778, val accuracy: 0.207000\n",
      "Loss: 2.276632, Train accuracy: 0.205222, val accuracy: 0.211000\n",
      "Loss: 2.249985, Train accuracy: 0.205778, val accuracy: 0.209000\n",
      "Loss: 2.205212, Train accuracy: 0.206222, val accuracy: 0.213000\n",
      "Loss: 2.097895, Train accuracy: 0.209000, val accuracy: 0.215000\n",
      "Loss: 2.309422, Train accuracy: 0.209667, val accuracy: 0.218000\n",
      "Loss: 1.934007, Train accuracy: 0.210333, val accuracy: 0.218000\n",
      "Loss: 2.100403, Train accuracy: 0.211444, val accuracy: 0.220000\n",
      "Loss: 2.038377, Train accuracy: 0.212444, val accuracy: 0.221000\n",
      "Loss: 2.025368, Train accuracy: 0.213444, val accuracy: 0.221000\n",
      "Loss: 2.185599, Train accuracy: 0.213778, val accuracy: 0.221000\n",
      "Loss: 2.146125, Train accuracy: 0.214222, val accuracy: 0.222000\n",
      "Loss: 2.204807, Train accuracy: 0.215111, val accuracy: 0.223000\n",
      "Loss: 2.383693, Train accuracy: 0.216000, val accuracy: 0.223000\n",
      "Loss: 2.243355, Train accuracy: 0.216556, val accuracy: 0.223000\n",
      "Loss: 2.191012, Train accuracy: 0.217222, val accuracy: 0.223000\n",
      "Loss: 2.253040, Train accuracy: 0.186667, val accuracy: 0.196000\n",
      "Loss: 2.271073, Train accuracy: 0.195000, val accuracy: 0.205000\n",
      "Loss: 2.144315, Train accuracy: 0.196778, val accuracy: 0.206000\n",
      "Loss: 2.367334, Train accuracy: 0.197444, val accuracy: 0.206000\n",
      "Loss: 2.152154, Train accuracy: 0.198778, val accuracy: 0.209000\n",
      "Loss: 2.220600, Train accuracy: 0.199333, val accuracy: 0.209000\n",
      "Loss: 2.185262, Train accuracy: 0.200333, val accuracy: 0.210000\n",
      "Loss: 2.283126, Train accuracy: 0.202333, val accuracy: 0.211000\n",
      "Loss: 2.309615, Train accuracy: 0.203444, val accuracy: 0.212000\n",
      "Loss: 2.230895, Train accuracy: 0.204667, val accuracy: 0.213000\n",
      "Loss: 2.201342, Train accuracy: 0.205111, val accuracy: 0.215000\n",
      "Loss: 2.151733, Train accuracy: 0.206111, val accuracy: 0.216000\n",
      "Loss: 2.172806, Train accuracy: 0.206333, val accuracy: 0.217000\n",
      "Loss: 2.058668, Train accuracy: 0.207000, val accuracy: 0.219000\n",
      "Loss: 2.125550, Train accuracy: 0.207667, val accuracy: 0.219000\n",
      "Loss: 2.143716, Train accuracy: 0.208333, val accuracy: 0.220000\n",
      "Loss: 2.234626, Train accuracy: 0.208778, val accuracy: 0.220000\n",
      "Loss: 2.190924, Train accuracy: 0.209111, val accuracy: 0.221000\n",
      "Loss: 2.161195, Train accuracy: 0.209000, val accuracy: 0.221000\n",
      "Loss: 2.127928, Train accuracy: 0.209556, val accuracy: 0.221000\n",
      "Loss: 2.257069, Train accuracy: 0.178333, val accuracy: 0.165000\n",
      "Loss: 2.224930, Train accuracy: 0.202667, val accuracy: 0.194000\n",
      "Loss: 2.188491, Train accuracy: 0.209333, val accuracy: 0.204000\n",
      "Loss: 2.106960, Train accuracy: 0.213111, val accuracy: 0.208000\n",
      "Loss: 2.272466, Train accuracy: 0.215111, val accuracy: 0.212000\n",
      "Loss: 2.382554, Train accuracy: 0.218444, val accuracy: 0.218000\n",
      "Loss: 2.316066, Train accuracy: 0.219000, val accuracy: 0.222000\n",
      "Loss: 2.206041, Train accuracy: 0.219667, val accuracy: 0.223000\n",
      "Loss: 2.234443, Train accuracy: 0.221000, val accuracy: 0.225000\n",
      "Loss: 1.922280, Train accuracy: 0.221889, val accuracy: 0.224000\n",
      "Loss: 2.052369, Train accuracy: 0.222333, val accuracy: 0.224000\n",
      "Loss: 2.252916, Train accuracy: 0.222667, val accuracy: 0.224000\n",
      "Loss: 2.070903, Train accuracy: 0.223444, val accuracy: 0.225000\n",
      "Loss: 2.212686, Train accuracy: 0.224222, val accuracy: 0.225000\n",
      "Loss: 2.247593, Train accuracy: 0.224556, val accuracy: 0.227000\n",
      "Loss: 2.134023, Train accuracy: 0.225111, val accuracy: 0.229000\n",
      "Loss: 2.093365, Train accuracy: 0.225333, val accuracy: 0.229000\n",
      "Loss: 2.290467, Train accuracy: 0.225556, val accuracy: 0.229000\n",
      "Loss: 2.104045, Train accuracy: 0.226222, val accuracy: 0.230000\n",
      "Loss: 2.120416, Train accuracy: 0.226222, val accuracy: 0.230000\n",
      "Loss: 2.302354, Train accuracy: 0.105000, val accuracy: 0.101000\n",
      "Loss: 2.307995, Train accuracy: 0.109778, val accuracy: 0.109000\n",
      "Loss: 2.300254, Train accuracy: 0.115000, val accuracy: 0.112000\n",
      "Loss: 2.298531, Train accuracy: 0.118556, val accuracy: 0.112000\n",
      "Loss: 2.309459, Train accuracy: 0.124444, val accuracy: 0.113000\n",
      "Loss: 2.391695, Train accuracy: 0.131111, val accuracy: 0.118000\n",
      "Loss: 2.312485, Train accuracy: 0.133333, val accuracy: 0.123000\n",
      "Loss: 2.287297, Train accuracy: 0.136667, val accuracy: 0.129000\n",
      "Loss: 2.318755, Train accuracy: 0.138444, val accuracy: 0.129000\n",
      "Loss: 2.283984, Train accuracy: 0.140778, val accuracy: 0.134000\n",
      "Loss: 2.285008, Train accuracy: 0.142333, val accuracy: 0.135000\n",
      "Loss: 2.263251, Train accuracy: 0.144778, val accuracy: 0.140000\n",
      "Loss: 2.298446, Train accuracy: 0.147000, val accuracy: 0.142000\n",
      "Loss: 2.212630, Train accuracy: 0.148556, val accuracy: 0.140000\n",
      "Loss: 2.314098, Train accuracy: 0.149889, val accuracy: 0.142000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.299848, Train accuracy: 0.151444, val accuracy: 0.141000\n",
      "Loss: 2.318161, Train accuracy: 0.153000, val accuracy: 0.142000\n",
      "Loss: 2.305902, Train accuracy: 0.153222, val accuracy: 0.142000\n",
      "Loss: 2.244971, Train accuracy: 0.154111, val accuracy: 0.142000\n",
      "Loss: 2.313547, Train accuracy: 0.155000, val accuracy: 0.142000\n",
      "Loss: 2.268741, Train accuracy: 0.152000, val accuracy: 0.131000\n",
      "Loss: 2.241638, Train accuracy: 0.166889, val accuracy: 0.143000\n",
      "Loss: 2.289532, Train accuracy: 0.171778, val accuracy: 0.152000\n",
      "Loss: 2.332772, Train accuracy: 0.176333, val accuracy: 0.159000\n",
      "Loss: 2.254451, Train accuracy: 0.179333, val accuracy: 0.162000\n",
      "Loss: 2.260202, Train accuracy: 0.181556, val accuracy: 0.172000\n",
      "Loss: 2.186881, Train accuracy: 0.182778, val accuracy: 0.179000\n",
      "Loss: 2.246296, Train accuracy: 0.184889, val accuracy: 0.182000\n",
      "Loss: 2.314828, Train accuracy: 0.186556, val accuracy: 0.181000\n",
      "Loss: 2.270169, Train accuracy: 0.187000, val accuracy: 0.185000\n",
      "Loss: 2.261766, Train accuracy: 0.188111, val accuracy: 0.188000\n",
      "Loss: 2.316689, Train accuracy: 0.188444, val accuracy: 0.189000\n",
      "Loss: 2.237998, Train accuracy: 0.188778, val accuracy: 0.190000\n",
      "Loss: 2.297470, Train accuracy: 0.189333, val accuracy: 0.191000\n",
      "Loss: 2.202179, Train accuracy: 0.190556, val accuracy: 0.190000\n",
      "Loss: 2.290705, Train accuracy: 0.191111, val accuracy: 0.193000\n",
      "Loss: 2.269493, Train accuracy: 0.191556, val accuracy: 0.192000\n",
      "Loss: 2.344288, Train accuracy: 0.191778, val accuracy: 0.194000\n",
      "Loss: 2.282146, Train accuracy: 0.191889, val accuracy: 0.193000\n",
      "Loss: 2.293591, Train accuracy: 0.192111, val accuracy: 0.193000\n",
      "Loss: 2.321399, Train accuracy: 0.088778, val accuracy: 0.090000\n",
      "Loss: 2.342780, Train accuracy: 0.098556, val accuracy: 0.095000\n",
      "Loss: 2.277954, Train accuracy: 0.107444, val accuracy: 0.109000\n",
      "Loss: 2.337416, Train accuracy: 0.115778, val accuracy: 0.119000\n",
      "Loss: 2.341754, Train accuracy: 0.124667, val accuracy: 0.129000\n",
      "Loss: 2.329383, Train accuracy: 0.131333, val accuracy: 0.137000\n",
      "Loss: 2.287305, Train accuracy: 0.136000, val accuracy: 0.142000\n",
      "Loss: 2.268576, Train accuracy: 0.140556, val accuracy: 0.150000\n",
      "Loss: 2.217302, Train accuracy: 0.142778, val accuracy: 0.152000\n",
      "Loss: 2.236385, Train accuracy: 0.144778, val accuracy: 0.158000\n",
      "Loss: 2.278162, Train accuracy: 0.147000, val accuracy: 0.161000\n",
      "Loss: 2.307648, Train accuracy: 0.148889, val accuracy: 0.158000\n",
      "Loss: 2.230357, Train accuracy: 0.150222, val accuracy: 0.159000\n",
      "Loss: 2.265940, Train accuracy: 0.150778, val accuracy: 0.163000\n",
      "Loss: 2.317826, Train accuracy: 0.151556, val accuracy: 0.165000\n",
      "Loss: 2.271892, Train accuracy: 0.152778, val accuracy: 0.166000\n",
      "Loss: 2.278738, Train accuracy: 0.153556, val accuracy: 0.166000\n",
      "Loss: 2.283807, Train accuracy: 0.154111, val accuracy: 0.167000\n",
      "Loss: 2.320965, Train accuracy: 0.154111, val accuracy: 0.166000\n",
      "Loss: 2.272072, Train accuracy: 0.153556, val accuracy: 0.165000\n",
      "best validation accuracy achieved: 0.713000\n",
      "[1.00e-03 1.00e-06 9.99e-01]\n"
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strength = [1e-4, 1e-5, 1e-6]\n",
    "learning_rate_decay = [0.999, 0.9, 0.88]\n",
    "hidden_layer_size = 175\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "best_hyperparams = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "\n",
    "hyperparams = np.array(np.meshgrid(learning_rates, reg_strength, learning_rate_decay)).T.reshape(-1,3)\n",
    "for hp in hyperparams:\n",
    "    model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = hidden_layer_size, reg = hp[1])\n",
    "    trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=hp[0], learning_rate_decay=hp[2])\n",
    "    loss, train_acc, val_acc = trainer.fit()\n",
    "    if best_val_accuracy == None:\n",
    "        best_val_accuracy = val_acc[-1]\n",
    "        best_classifier = trainer\n",
    "        loss_history = loss\n",
    "        train_history = train_acc\n",
    "        val_history = val_acc\n",
    "        best_hyperparams = hp\n",
    "    elif val_acc[-1] > best_val_accuracy:\n",
    "        best_val_accuracy = val_acc[-1]\n",
    "        best_classifier = trainer\n",
    "        loss_history = loss\n",
    "        train_history = train_acc\n",
    "        val_history = val_acc\n",
    "        best_hyperparams = hp\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a1c97eb860>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net test set accuracy: 0.657000\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = best_classifier.compute_accuracy(test_X, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
